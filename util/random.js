// => class _Random
// base class for all random variables
// defines common properties, methods, and hooks
// underscore-prefixed to reserve name for global initializer
class _Random {
  constructor() {
    // TODO: define and document properties ...
  }

  // => _Random.init(...args)
  // creates new random variable
  // can be invoked on any subclass as `_Type.init`
  // seals created object; use `new _Type` for unsealed
  static init(...args) {
    return Object.seal(new this.prototype.constructor(...args))
  }

  // => _Random.type
  // type of random variable
  // defined on both class (static) and instances
  static get type() {
    return this.prototype.constructor.name
  }
  get type() {
    return this.constructor.type
  }

  // => _Random.domain
  // domain of random variable
  // defined on both class (static) and instances
  static get domain() {
    return this._domain() // see hook below
  }
  get domain() {
    return this._domain(this.params)
  }

  // …/setters for samples and weights can set fixed values for random variables. This is most useful to specify observed (or assumed) outputs of child processes to infer parameters generated by parent processes. In addition, setters for index and value can fix the index and/or value for enumeration purposes.

  // set fixed (observed) values for random variable
  // can be optionally weighted by also setting weights
  // resets all associated state; should be set rarely
  // equivalent to calling method .sample(xJ)
  set samples(xJ) {
    const τ = this
    τ.reset()
    τ.xJ = xJ
    τ.J = xJ?.length || 0
    if (τ.stats) τ.stats.samples++
  }
  reset() {
    const τ = this
    τ.J = 0
    τ.xJ = undefined
    τ.jJ = undefined
    // reset buffers, cache, weights, posterior, index
    τ._xJ = τ._wJ = τ._jJ = τ._φJ = τ._cache = undefined
    τ.wJ = τ.wj_sum = undefined
    τ.φJ = undefined
    τ.j = undefined
    τ.observed = undefined // since J=0
    if (τ.M) τ.m = 0 // reset move tracking
  }

  // set optional weights ∝ q(x)/p(x), p = sample distribution
  // enables sample to behave as q(x) with reduced efficiency
  // default weights are uniform, with maximal efficiency
  // resets all associated state; should be set rarely
  // equivalent to calling method .weight(wJ)
  set weights(wJ) {
    const τ = this
    check(τ.xJ, 'weights set before samples')
    check(wJ.length == τ.J, 'weights wrong size')
    τ.wJ = wJ
    τ.wj_sum = sum(wJ)
    check(τ.wj_sum > 0, 'invalid weights')
    τ.clear_cache()
    if (τ.stats) τ.stats.weights++
  }

  // set fixed index/value for random variable
  // must be set to undefined to reset
  set index(j) {
    check(j >= 0 && j < this.J, 'invalid index')
    this.j = j
  }

  // set fixed value for random variable
  // must be set to undefined to reset
  set value(v) {
    this.v = v
  }

  // …/getters define aliases for internal properties, except for index and value, which are non-trivial stochastic (thus uncached) methods defined as properties for convenience.

  get size() {
    return this.J
  }
  get samples() {
    return this.xJ
  }
  get weights() {
    return this.wJ
  }
  get weight_sum() {
    return this.wj_sum
  }
  get index() {
    const τ = this
    check(τ.J > 0, 'no samples')
    if (τ.J == 1) return 0
    if (τ.j >= 0) return τ.j // fixed index
    if (!τ.wJ) return uniform_int(τ.J)
    return discrete(τ.wJ, τ.wj_sum)
  }
  get value() {
    // weighted-random value
    const τ = this
    // τ.v==null means "cached value" for joint sampling
    if (τ.v === null) {
      τ.v = undefined
      return (τ.v = τ.value)
    }
    if (τ.v !== undefined) return τ.v // fixed value
    if (τ.J) return τ.xJ[τ.index] // pre-sampled value
    return τ._value(τ.θ._sample()) // random value
  }

  //…/cached properties are those stored under cache, e.g. cache.expensive_reusable_result. Cache is cleared (={}) automatically whenever samples or weights are modified. Convenience method cached(key,func) can compute cached properties as needed. Convenient accessors are also provided for many built-in cached properties such as min, max, mean, ...

  // "weighted" sample means weights that deviate more than 0.001x
  get weighted() {
    return this.cached('weighted', τ => {
      if (!τ.wJ || !τ.wj_sum || τ.J == 0) return false
      const w_mean = τ.wj_sum / τ.J
      const [w_min, w_max] = [0.999 * w_mean, 1.001 * w_mean]
      return !every(τ.wJ, w => w > w_min && w < w_max)
    })
  }
  get unweighted() {
    return !this.weighted
  }
  // NOTE: properties/methods below require unweighted samples
  get min() {
    return this.cached('min', τ => min(τ.xJ), true)
  }
  get max() {
    return this.cached('max', τ => max(τ.xJ), true)
  }
  get min_max() {
    return this.cached('min_max', τ => [τ.min, τ.max], true)
  }
  get mean() {
    return this.cached('mean', τ => mean(τ.xJ), true)
  }
  get stdev() {
    return this.cached('stdev', τ => stdev(τ.xJ), true)
  }
  get median() {
    return this.cached('median', τ => median(τ.xJ), true)
  }
  quantiles(...qQ) {
    qQ = flatten([...qQ])
    return this.cached('quantiles-' + qQ, τ => quantiles(τ.xJ, qQ), true)
  }

  // NOTE: properties/methods below work w/ weighted samples
  // mode and antimode w.r.t. posterior weights (φJ)
  get mode() {
    return this.cached('mode', τ => this.modef(w => w))
  }
  get antimode() {
    return this.cached('antimode', τ => this.modef(w => -w))
  }
  modef(f) {
    const wX = this.postX // counts if unweighted
    const xK = keys(wX),
      wK = values(wX)
    const w_mode = maxf(wK, f)
    const r = uniform_int(count(wK, w => f(w) == w_mode))
    for (let k = 0, n = 0; k < wK.length; ++k)
      if (f(wK[k]) == w_mode && r == n++) return xK[k]
  }
  get counts() {
    return this.cached('counts', τ => counts(τ.xJ))
  }
  // NOTE: weightsX aggregates by value so counts become weights
  get weightsX() {
    return this.cached('weightsX', τ => {
      if (!τ.wJ) return τ.counts // treat counts as weights
      const wX = {}
      each(τ.xJ, (x, j) => (wX[x] = (wX[x] || 0) + τ.wJ[j]))
      return wX
    })
  }
  // posterior (φJ) aggregated by value
  get postX() {
    return this.cached('postX', τ => {
      if (!τ.φJ) return τ.counts // treat counts as posterior weights
      const φX = {}
      each(τ.xJ, (x, j) => (φX[x] = (φX[x] || 0) + τ.φJ[j]))
      return φX
    })
  }
  get probs() {
    return this.cached('probs', τ => {
      if (!τ.wJ) return new Array(τ.J).fill(1 / τ.J)
      const z = 1 / τ.wj_sum
      return τ.wJ.map(w => w * z)
    })
  }
  probs_at(xJ) {
    const wX = this.weightsX // counts if unweighted
    const z = 1 / if_defined(this.wj_sum, this.J)
    return xJ.map(x => (wX[x] || 0) * z)
  }

  // negative log posterior probability (NLP) of _distinct_ samples
  get nlp() {
    return this.cached('nlp', τ => {
      if (τ.jJ) {
        // aggregate over indices jJ using _wJ as buffer
        if (!τ._wJ) τ._wJ = array(τ.J)
        τ._wJ.fill(0)
        each(τ.jJ, (jj, j) => (τ._wJ[jj] += τ.wJ[j]))
        return -sumf(τ.φJ, (φ, j) => (τ._wJ[j] ? φ * τ._wJ[j] : 0)) / sum(τ._wJ)
      }
      return -sumf(τ.φJ, (φ, j) => (τ.wJ[j] ? φ * τ.wJ[j] : 0)) / τ.wj_sum
    })
  }

  // ess gave excellent agreement with empirical cdf/quantiles
  // we distinguish (weighted) ess from essu and essr below
  get ess() {
    return this.cached('ess', τ => {
      if (!τ.wJ) return τ.essu // no weights so ess=essu
      if (τ.jJ) {
        // aggregate over indices jJ using _wJ as buffer
        if (!τ._wJ) τ._wJ = array(τ.J)
        τ._wJ.fill(0)
        each(τ.jJ, (jj, j) => (τ._wJ[jj] += τ.wJ[j]))
        return ess(τ._wJ)
      }
      return ess(τ.wJ)
    })
  }
  // essu ("unweighted" ess) ignores weights
  // adjusts sample size only for duplication in resampling
  // resampling can improve ess only up to <~1/2 essu
  // resampling _shrinks_ essu by ~1/2 or ~k/(k+1) toward ess=1
  get essu() {
    return this.cached('essu', τ => {
      if (!τ.jJ) return τ.J
      if (!τ._wJ) τ._wJ = array(τ.J) // used to count jJ
      τ._wJ.fill(0)
      each(τ.jJ, jj => τ._wJ[jj]++)
      return ess(τ._wJ)
    })
  }
  // essr is just ess/essu, a natural measure of weight skew
  get essr() {
    return this.ess / this.essu
  }

  // kolmogorov-smirnov statistic against (cached) target
  // collisions are resolved randomly (as is default for ks)
  // ess is used instead of length for weighted samples
  // weighted ks is used to avoid resampling bias
  // collisions are allowed for exact (⟹discrete) target
  // (otherwise ks cdf is inaccurate, esp. for small support)
  // (as can be seen from sensitivity of ks to support size)
  // (w/ collisions the ks cdf becomes lower-bound ⟹ alpha upper-bound ⟹ 1/alpha lower-bound that can often hit ~zero)
  ks() {
    return this.cached('ks', τ => {
      const t = τ.target
      return ks(
        τ.samples,
        t.samples,
        t.exact,
        τ.weighted ? τ.weights : undefined,
        τ.weighted ? τ.weight_sum : undefined,
        t.weights,
        t.weight_sum
      )
    })
  }
  // regular cdf (w/o small-sample correction) works well
  // exact case needs one-sided cdf, equivalent to size 2n on both sides since 2*n*m/(n+m)->2n as m->∞ and 2*2n*2n/(2n+2n)=2n
  ks_cdf() {
    return this.cached('ks_cdf', τ => {
      const t = τ.target
      if (t.exact)
        // need one-sided cdf w/ n=m->2n (see above)
        return ks_cdf_at(2 * τ.ess, 2 * τ.ess, τ.ks())
      return ks_cdf_at(τ.ess, t.ess, τ.ks())
    })
  }
  ks_alpha() {
    return clip(1 - this.ks_cdf())
  }

  // mks (move ks) metric used as a convergence/mixing indicator
  get mks() {
    return this.cached('mks', τ => {
      if (!(τ.m >= τ.M)) return inf
      // rotate history so m=0 and we can split in half at M/2
      const _xM = array(τ.M),
        _yM = array(τ.M),
        m = τ.m % τ.M
      copy_at(_xM, τ.xM, 0, m)
      copy_at(_xM, τ.xM, τ.M - m, 0, m)
      copy_at(_yM, τ.yM, 0, m)
      copy_at(_yM, τ.yM, τ.M - m, 0, m)
      return -Math.log2(ks_alpha(_xM.slice(0, τ.M / 2), _yM.slice(-τ.M / 2)))
    })
  }

  // returns inferred or assumed target sample from cache
  // returns undefined if missing; can be used to test existence
  get target() {
    return this.cache2.target?.adjust(this)
  }

  // computes ("infers") target sample by updating w/ defaults
  // resulting target sample is cached on instance-level cache
  infer(size, options) {
    return this.sample(size).update(options).assume()
  }
  // sets ("assumes") current (or given) random sample as "target"
  // type should be 'exact' iff samples=support & weights∝posterior
  // typically used for inference performance evaluation purposes
  assume(target = this, type = 'sample') {
    check(
      ['sample', 'exact', 'median'].includes(type),
      `invalid target type '${type}'`
    )
    let t = target
    t = this.cache2.target = {
      type,
      ess: type == 'exact' ? t.size /*full ess*/ : t.ess /*≤t.size*/,
      exact: type == 'exact', // ⟺ samples=support & weights∝posterior
      samples: clone(t.samples),
      weights: t.weighted ? clone(t.weights) : undefined,
      weight_sum: t.weighted ? t.weight_sum : undefined,
    }
    // set up convenience functions for random (re)sampling
    const size = t.samples.length
    t.values = (J = size) => Random_(t.samples, t.weights).values(J)
    t.sample = (J = size, allow_resampling = false) => {
      if (J == size && !t.weights) return t.samples
      check(
        allow_resampling,
        'size mismatch and/or weighting requires resampling but allow_resampling==false'
      )
      return t.values(J)
    }
    // set up "adjustment" function for "dynamic" target types
    if (type == 'median') {
      check(size == 1 && !t.weights, 'invalid target')
      // adjust to match median on a random sample ...
      const t_median = t.samples[0]
      t.adjust = X => {
        if (X.size != t.samples.length) t.samples = array(X.size)
        // NOTE: we resample to allow weights while matching median
        const xK = X.values()
        const x_median = median(xK)
        fill(t.samples, k => xK[k] + 2 * (t_median - x_median) * uniform())
        t.ess = X.ess
        return t
      }
    } else {
      t.adjust = () => t
    }
    return this // for chaining
  }

  // default cache is sample/weight-level and auto-managed
  get cache() {
    return this._cache ?? (this._cache = {})
  }
  cached(k, f, require_unweighted) {
    if (require_unweighted)
      check(this.unweighted, 'unweighted sample required for ' + k)
    return this.cache[k] ?? (this.cache[k] = f(this))
  }
  clear_cache() {
    if (this._cache) this._cache = {}
  }

  // second-level cache is instance-level and self-managed
  get cache2() {
    return this._cache2 || (this._cache2 = {})
  }
  cached2(k, f, require_unweighted) {
    if (require_unweighted)
      check(this.unweighted, 'unweighted sample required for ' + k)
    return this.cache2[k] ?? (this.cache2[k] = f(this))
  }
  clear_cache2() {
    if (this._cache2) this._cache2 = {}
  }
}

const Random = (...args) => _Random.init(...args)

const is_random = x => x instanceof _Random

const value = x => (is_random(x) ? x.value : x)

const value_deep = x => (is_random(x) ? value_deep(x.value) : x)
