#util/sample/examples/6 Sample `A∈(0,1)`, `B∈(a,1)`, `C∈(a,b)`. Weight samples by _likelihood_ `P(C'∈(.495,.505)|a,b)`, equivalent to _confining a fork_ `C'|a,b` of `C|a,b` w/o sampling it. The effect on `A` and `B` is identical to confining `C∈(.495,.505)`. However the effect on `C` is _much softer_ and can be interpreted as the _prediction_ for `C` for a _future run_ having _observed_ `C∈(.495,.505)` _in a past run_. Weighting requires more information (i.e. the likelihood or other desired weight) and can be more error-prone but is more general and powerful than both confinement and conditioning: it naturally weights samples inside domain and can also naturally guide samples outside domain wherever (or whenever) weights are positive, `(.495,.505)⋂(A,B)≠∅` in this example.
```js:js_input

let a = sample(between(0, 1))
let b = sample(between(a, 1))
let c = sample(between(a, b))
let p_c_ab = clip(min(.505,b) - max(.495,a)) / (b-a) // P(C'∈(.495,.505)|a,b)
weight(log(p_c_ab))

```
```js:js_removed

const targets = transpose_objects(random_array(1000, () => {
  const a = random()
  const b = random_uniform(a, 1)
  const c = random_uniform(a, b)
  // we can accept w/ prob. p_c_ab OR condition on fork c'|a,b
  // const p_c_ab = clip(min(.505,b) - max(.495,a)) / (b-a)
  // if (random_boolean(p_c_ab)) return {a, b, c}
  const cc = random_uniform(a, b) // fork c'|a,b (of c|a,b)
  if (cc > .495 && cc < .505) return {a, b, c}
}, defined))

const _sample_options = { 
  stats:'mks tks ess elw r t',  
  targets,
  table:true,
  plot:true,
}

```
#_util