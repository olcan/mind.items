#util/random/weight method sets or updates (_reweights_) sample weights to help guide samples towards a #posterior_sample. Some interpretations:
- [Discrete measure](https://en.wikipedia.org/wiki/Discrete_measure) Î¼=âˆ‘w(Î¸â‚™)Î´(Î¸â‚™,Î¸) ~ wÂ·P where Î¸â‚™~P is sample distribution.
  - Measure wÂ·P can be implicit, e.g. as stationary dist. of markov chains.
- [Importance weights](https://en.wikipedia.org/wiki/Importance_sampling) for [efficient](https://en.wikipedia.org/wiki/Efficiency_(statistics)) and [bias/variance-controlled](https://en.wikipedia.org/wiki/Biasâ€“variance_tradeoff) estimators of the form Î·=(1/N)âˆ‘f(Î¸â‚™)w(Î¸â‚™) â†’ E[f;wÂ·P]=âˆ«f(Î¸)w(Î¸)p(Î¸)dÎ¸. Some [Posterior](#posterior_inference) [MVUEs](https://en.wikipedia.org/wiki/Minimum-variance_unbiased_estimator):
  - MVUE for E[f;Q] is [achieved by](https://en.wikipedia.org/wiki/Importance_sampling#Application_to_simulation) p(Î¸)âˆq(Î¸)|f(Î¸)| and w(Î¸)=q(Î¸)/p(Î¸)âˆ1/|f(Î¸)|.
  - MVUE for Q(A)=E[ğŸ™[Î¸âˆˆA];Q] is achieved by p(Î¸)âˆq(Î¸|A) and w(Î¸)=1.
  - MVUE for Q(Î¸) is achieved by p(Î¸)=q(Î¸) and w(Î¸)=1.
    - If p(Î¸)=Ï€(Î¸) _fixed_, then w(Î¸)=q(Î¸)/Ï€(Î¸), but Var[Î·] âˆ q(Î¸)Â²/Ï€(Î¸)Â².
    - Can lead to degenerate weights, e.g. in likelihood-weighted prior.
    - Resampling resets w=1 but turns variance into bias.
    - Markov chains can be defined for Pâ†’Q.
- Finite-sample approximation of likelihood
  - Observation indicator (0/1) weights lead to [rejection sampling](https://en.wikipedia.org/wiki/Rejection_sampling)
  - Integrating indicators over infinite sample leads to likelihood
- Non-likelihood weights as data/model augmentation