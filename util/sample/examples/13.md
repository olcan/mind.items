#util/sample/examples/13
```js:js_input

sample_array(A, alloc, summand(1), 'crypto stocks bonds reits')
let x = simulate({ alloc, prices:ones(A), value:1 }, 10, events)
maximize(x.value, .9)

```
```js:js_removed

// TODO: we are repeatedly fork-conditioning (or likelihood-weighting) on x being in top (1-q) among current samples (unweighted but subject to weights already baked into sample); note initial conditioning/weighting creates a posterior, and then we repeat with that posterior, until there is convergence such that runs that satisfy condition are not meaningfully different from those who do not, and therefore the quantile value is no longer improving across iterations; another way to look at this is to say that we are conditioning on x being in top (1-q) not under prior but under _posterior (predictive)_, which is only defined circularly, but iterating should converge to when the condition is no longer informative about sampled variables (allocation, etc), because their distribution is the same on both sides of the condition, i.e. x being in top r(1-q) is _purely_ due to "sampling noise"
//
// note this _could_ be viewed as optimizing an asymptotic posterior quantile q, but quantile values are NOT comparable across different choices of q, because each q is under a _different_ (asymptotic) posterior predictive
//
// an advantage of this way of defining the optimization is that we are guaranteed that the "optimal outcome" has probability at least (1-q), i.e. we are not conditioning on a meaningless tail event that would simply favor samples w/ higher variance in the outcome
//
// informativeness of the quantile condition is an interesting concept: does any subset of the current population assign probability P(x>=xq)>(1-q) to quantile value xq of the current population? if so, then posterior mass is shifted in that direction, and we stop when the population is homogeneous in that regard, at which point P(x>=xq)=(1-q) and event (x>=xq) is non-informative and driven only by sampling noise!
//
// TODO: add these notes to method comments or other comments, enable mixing of optimization w/ other weights w/ their rn incremented adaptively as usual
//
// TODO: consider posterior predictive ("forked") weighting in all cases? need to test in earlier examples, and may be worth having as a global option as a simpler alternative to forking manually

const A = 4 // assets
const alloc = [1,0,0,0] // allocation (optimal)
const μA = [.05, .025, .005, .0125] // mean change (relative) per period
                                    // crypto (0) dominates, more at higher q
// const μA = [.04, .05, .05, .05] // <- 0 @q=.9, 1 @q=.8
// const μA = [.05, .05, .05, .05] // <- 0 @q>.7, 3 @q<.4
const σA = [.1, .05, .01, .025] // standard deviation in change per period
const dA = array(A, a=> μA[a]-.5*σA[a]**2)
const reprice = _do(x=>{
  apply(x.prices, (x,a)=> x * exp(dA[a] + σA[a]*random_normal()))
  x.value = sum(A, a=> x.alloc[a] * x.prices[a])
})
const events = name_events(()=>reprice)

const _sample_options = { 
  params:{A},
  stats:'ess wsum r t ua.crypto median.x.value',
  max_time: 3000,
  // max_time: 5000,
  // min_time: 4000,
  // opt_time: 2000,
  // min_ess: 900,
  table:true,
  plot:true,
  // log:true,
  // size:1, // debug
}

```
#_util
