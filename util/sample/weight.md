#util/sample/weight function defines weighted models `Q ‚àù P√óW`
- defines _posterior_ in general sense of a _weighted prior_
  - interpretable as conditional `P(X|c)` in two cases:
    - _likelihood weights_ `W(X) ‚àù P(c|X) = E[ùüô(c|X)]`
      - equivalent to _past-conditioning_ `P(X|c_past)` on past run(s)
    - _indicator weights_ `W(X) ‚àù ùüô(c|X)` 
      - equivalent to conditioning `P(X|c_now)` on same/current run
- unnormalized weights `w(x)` specified on finite sample of points `(x‚Çô)ÔΩûP`
  - points `(x‚Çô)` = runs of sampling context `sample(function)`
  - points `(x‚Çô)ÔΩûP` are from prior `P` instead of posterior `Q`
    - weights `w(x) = q(x)/p(x)` necessary for _unbiased integrals_ w.r.t. Q
      - note `E[w;P]=1` whereas `E[W;P]=1/N` since `W` is normalized
      - weights `w=q/p` are [MVUE](https://en.wikipedia.org/wiki/Minimum-variance_unbiased_estimator) for any `E[f;Q]` for _fixed_ `p(x)`
        - [general MVUE (variable p)](https://en.wikipedia.org/wiki/Importance_sampling#Application_to_simulation) is `p‚àùq|f|` and `w=q/p‚àù1/|f|`
    - weights _increase variance_ `√ó N√óE[W¬≤] ‚àà [1,N]` to remove bias
      - equivalently _reduce (effective) sample size_ `√ó 1/(N√óE[W¬≤]) ‚àà [1/N,1]`
        - `ess` can be derived as `N√óVar(wtd_mean)/Var(mean)` for i.i.d. r.v.
        - `ess` considered approximation of `N√óVar(target)/MSE(sample)`
          - `target` ve `sample` are estimators based on `Q` vs `P`
          - bias in `MSE(sample)` is due to self-normalization for `Q`
            - bias¬≤ shrinks as `O(N‚Åª¬≤)` vs `O(N‚Åª¬π)` for variance
          - see [Rethinking the Effective Sample Size](https://arxiv.org/abs/1809.04129) for details
- **Goal**: _unweighted unduplicated_ posterior sample `(x‚Çô)ÔΩûQ`
  - let `Qi ‚Üí Q, i=0,1,‚Ä¶` denote _target sequence_, starting at `Q0=P`
    - goal is to transition samples `(x‚Çô)ÔΩûP ‚Üí (y‚Çô)ÔΩûQ`
    - small changes `Qi ‚Üí Qi+1` avoid extreme weights
      - easier to transition samples `(x‚Çô)‚ÜíQi ‚Üí (z‚Çô)‚ÜíQi+1`
  - let `Wi ‚Üí W, i=0,1,‚Ä¶` denote corresponding _weight sequence_ `Wi‚àùQi/P`
    - `W0` can be used to define a (pre-)weighted prior `Q0 ‚àù P√óW0`
    - `Wi` can be used to define target sequence `Qi ‚àù P√óWi`
      - should be chosen to avoid extreme ratios `W·µ¢/W·µ¢‚Çã‚ÇÅ`
        - `W/W‚ÇÄ` can still be extreme (and typically is)
      - common methods:
        - weight exponent (or log-weight multiplier)
        - relaxed (progressively tightened) condition indicator
      - relevant sources, terms:
        - [Sequential Monte Carlo](https://en.m.wikipedia.org/wiki/Particle_filter) (SMC), a.k.a. Particle Filter
        - [Approx. Bayesian Comp.](https://en.wikipedia.org/wiki/Approximate_Bayesian_computation) (ABC), a.k.a. Likelihood-free Inference
          - [ABC Samplers](https://arxiv.org/abs/1802.09650), esp. Algorithm 8 (ABC-SMC).
  - let `w` denote _internal weights_, starting at `w‚àùW0`
    - `(x‚Çô)ÔΩûP` considered "distinct" in sense of independence
    - `essu=n`, `ess` depends on `W0` (`=n` if `W0` uniform)
      - `ess` tracks weights+counts, `essu` counts only (see below)
        - can have `ess>essu` or `ess<essu` based on relative skew
      - initial counts=1 since all samples distinct/independent
  - `w` can be adjusted by `√ó W·µ¢/W·µ¢‚Çã‚ÇÅ, i=1,2,‚Ä¶` in _reweighting_ steps
    - should shrink `ess` but does not affect `essu`
      - shrinks less as weights converge `w ‚âà W·µ¢/W·µ¢‚Çã‚ÇÅ ‚Üí W/W = 1`
      - increase in `ess` unlikely for sensible sequence `Wi`
    - should be idempotent at same `i` since `Wi/Wi=1`
    - should be convergent since `W·µ¢/W·µ¢‚Çã‚ÇÅ ‚Üí W/W = 1`
    - can accumulate as `W·µ¢/W·µ¢‚Çã‚ÇÅ √ó W·µ¢‚Çã‚ÇÅ/W·µ¢‚Çã‚ÇÇ √ó ‚Ä¶ = W·µ¢/W‚±º`
      - accumulated weights `W·µ¢/W‚±º, j<i-1` can become extreme ...
  - `w` can be reset to `w(x)=1` in _resampling_ steps
    - prevents extreme weights `W·µ¢/W‚ÇÄ` or `W·µ¢/W‚±º, j<i-1`
    - shrinks `essu` by `~1/2`, or `~k/(k+1), k=1,2,‚Ä¶` if repeated
    - ensures `ess=essu` since weights are reset (only counts remain)
      - `ess‚âàessu/2` w.r.t. pre-resample `essu`
      - more likely to improve `ess` if `ess<essu/2`, hurt if `ess>essu/2`
        - i.e. resampling should be avoided when `ess>essu/2` (see below)
    - can be interpreted as _global moves_, restricted only by sample
      - small-weight samples can _jump globally_ to large-weight points
    - converts weights `w(x)` to random counts w/ duplication (count‚â•2)
      - duplication in sense of dependence/correlation, not value/equality
      - weights `Wi` (and `W·µ¢‚Çã‚ÇÅ,W·µ¢‚Çã‚ÇÇ,‚Ä¶`) still guide move steps
    - considered _unweighted_ sample despite counts ‚â° integer weights
      - counts are about dependence/correlation, not value/equality
      - counts should be factored into _both_ `ess` and `essu`
      - key difference: count‚â•2 can be converted to distinct samples ...
  - samples `(x‚Çô)` can be _moved_ along markov chain `(x‚Çô)‚ÜíQi`
    - duplicates (correlates) can be _deduplicated_, reducing counts
      - deduplication in sense of (eventual) independence/decorrelation
      - necessary but not sufficient for _decorrelation_ (see below)
    - can improve both `essu` and `ess` as counts are reduced
      - does not affect weights `w` (aside from multiplicity in sample)
        - implies `ess/essu` should not change much
      - `ess` improvement depends on weights of deduplicated samples
        - best case is `+1` per move for uniform weights (e.g. post-resample)
        - worst case is `~0` for ~zero-weight samples
    - mixing/convergence is criticial to achieve goal (see above)
      - requires deduplication _and decorrelation_ of samples
        - deduplication is necessary but not sufficient
        - decorrelation depends on proposals and acceptance
          - global = less auto-correlated = harder to accept
          - local = more auto-correlated = easier to accept
        - can fail to move/deduplicate due to low acceptance rate
        - can fail to move sufficiently to decorrelate
          - can happen even after many (accepted) local moves
        - can fail to improve `essu` (and `ess`) sufficiently
          - could lead to a "spiral" `ess‚Üí1` w/ highly skewed reweights and frequent resamples that further reduce `essu‚Üíessu/2`
        - may be subject to slow-moving or even non-moving samples
          - these are problematic in theory but may be inevitable
      - ks diagnostics (esp. between independent samples) important
  - `Wi` are thus iteratively "baked into" sample as `(x‚Çô)‚ÜíQi`

... resume here ...

- resampling forces uniform weights by converting weights ‚Üí counts
  - smart resampling _can increase ess gradually to its maximum_
    - maximum ess‚ÜíJ requires uniform weights from resampling
    - once target is stable, essr stays ~1 after reweighting
    - effective moves s.t. essu‚ÜíJ can then allow ess‚Üíessu‚ÜíJ
    - resampling rule `essr < essu/J` can be sufficient
      - assuming move rule ensures `essu>J/2`, see below
      - `essr<clip(essu/J,.5,1)` robust to move rule and numerical issues
- moving attempts to move samples ÔΩû current target
  - moving rule `essu<J/2 || accepts<J` can ensure reasonable movement while allowing some slow-moving or even non-moving samples

- two hypothetical scenarios:
  - initial target is final posterior (`_weight` ~likelihood)
    - initial weights (~likelihood) will be extremely skewed (= low ess)
    - resampling will restore uniform weights but duplicate heavily (= still low ess)
    - burden is on move steps to increase ess & move samples to target
      - additional reweighting keeps uniform weights since target is final
      - additional resampling w/ uniform weights can only hurt ess
        - can be avoided easily based on ess (e.g. < 1/2 essu)
      - many move steps may be needed to ensure mixing w/ large ess
        - move steps may be interleaved with reweighting steps
  - initial target is ~prior (`_weight` ~uniform)
    - evolves from ~prior to ~posterior (`_weight` ~likelihood)
    - target evolution can help maintain samples ÔΩû target w/ good (~1/2) ess
      - may require fewer or faster/simpler (e.g. symmetric) moves overall
    - process should still be robust to advancing target without full mixing
      - markov chain mixing diagnostics should be used at least in outer loop