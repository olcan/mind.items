#util/sample/weight function defines weighted models `Q ∝ P×W`
- defines _posterior_ in general sense of a _weighted prior_
  - interpretable as conditional `P(X|c)` in two cases:
    - _likelihood weights_ `W(X) ∝ P(c|X) = E[𝟙(c|X)]`
    - _indicator weights_ `W(X) ∝ 𝟙(c|X)`
- unnormalized weights `w(x)` specified on finite sample of points `(xₙ)～P`
  - points `(xₙ)` = executions of sampling context `sample(function)`
  - points `(xₙ)～P` are from prior `P` instead of posterior `Q`
    - weights `w(x) = q(x)/p(x)` necessary for _unbiased integrals_ w.r.t. Q
      - note `E[w;P]=1` whereas `E[W;P]=1/N` since `W` is normalized
      - weights `w=q/p` are [MVUE](https://en.wikipedia.org/wiki/Minimum-variance_unbiased_estimator) for any `E[f;Q]` for _fixed_ `p(x)`
        - [general MVUE (variable p)](https://en.wikipedia.org/wiki/Importance_sampling#Application_to_simulation) is `p∝q|f|` and `w=q/p∝1/|f|`
    - weights _increase variance_ `× N×E[W²] ∈ [1,N]` to remove bias
      - equivalently _reduce (effective) sample size_ `× 1/(N×E[W²]) ∈ [1/N,1]`
        - can be derived as `N×Var(wtd_mean)/Var(mean)` for i.i.d. r.v.
        - considered approximation of `N×Var(target)/MSE(sample)`
          - `target` ve `sample` are estimators based on `Q` vs `P`
          - bias in `MSE(sample)` is due to self-normalization for `Q`
            - bias² shrinks as `O(N⁻²)` vs `O(N⁻¹)` for variance
          - see [Rethinking the Effective Sample Size](https://arxiv.org/abs/1809.04129) for details
- **Goal**: _unweighted unduplicated_ posterior sample `(xₙ)～Q`
  - let `Qi->Q, i=0,1,…` denote _target sequence_, starting at `Q0=P`
    - common in [Sequential Monte Carlo](https://en.m.wikipedia.org/wiki/Particle_filter) algorithms
  - let `Wi->W, i=0,1,…` denote corresponding _weight sequence_ `Wi∝Qi/P`
    - `W0` can be used to define a (pre-)weighted prior `Q0∝P×W0`
    - `Wi` can be used to define target sequence `Qi∝P×Wi`
      - may involve a weight multiplier/exponent
      - may involve an augmented condition/indicator
      - common in [Approximate Bayesian Computation](https://en.wikipedia.org/wiki/Approximate_Bayesian_computation) (ABC), a.k.a. Likelihood-free Inference (LFI). For technical background and derivation, see [ABC Samplers](https://arxiv.org/abs/1802.09650) and in particular the discussion leading up to Algorithm 8 (ABC-SMC).
  - let `w` denote variable _internal weights_, starting at `w∝W0`
  - `w` can be adjusted by `× Wi/Wi-1, i=1,2,…`
  - `w` can be reset to `w=1` by _resampling_ (w/ duplication)
  - samples `(xₙ)` can be _moved_ along markov chain `(xₙ)→Qi`
  - can be interpreted as [Sequential Monte Carlo](https://en.m.wikipedia.org/wiki/Particle_filter), a.k.a. a Particle Filter

#random/methods/update/notes
- weights `wJ` are NOT same as `_weight`
  - relative weight of NEXT target under CURRENT target
  - target is `φJ=prior*_weight`, sometimes called posterior
  - reweighting _assumes_ samples ～ current target
    - if true, guarantees unbiased integrals w.r.t. next target
  - reweighting w/ same target should leave `wJ` unchanged
- at initial sampling, target is "prior" and weights uniform
  - non-uniform initial weights are allowed if sample is NOT from prior
  - in that case `_sample_weighted` must be set to true
- reweighting can shift target from current to next
  - if target does not shift, weights are unchanged
  - once target=posterior, weights should remain unchanged
  - if target shifts too much, weights can become heavily skewed
    - this can happen over multiple steps if no resampling in between
  - skewed weights cause high integral variance and small ess
- resampling forces uniform weights by converting weights → counts
  - samples ∝ weights ⇒ ～ _discrete_ approximation to current target
  - can be interpreted as _global_ moves, restricted by current sample
    - small-weight samples can _jump_ globally to higher-weight ones
  - uniform weights minimize integral variance but introduce/increase bias
    - bias is random, due to sampling, and _baked into_ sample
  - _hurts ("shrinks") essu_ by factor ~1/2, or ~k/(k+1) for k'th resampling
  - _can improve ess_ by matching essu _after shrinkage_
    - guarantees ess=essu (⟹ esr=1) _post-shrinkage_
    - can also hurt ess, which can be _higher_ than essu
      - implies new weights _less skewed_ since last resampling
  - resampling should be avoided when essr > 1/2
    - more likely to hurt ess AND essu due to duplication
  - smart resampling _can increase ess gradually to its maximum_
    - maximum ess→J requires uniform weights from resampling
    - once target is stable, essr stays ~1 after reweighting
    - effective moves s.t. ess→J can then allow ess→essu→J
    - resampling rule `essr < essu/J` can be sufficient
      - assuming move rule ensures `essu>J/2`, see below
      - `essr<clip(essu/J,.5,1)` robust to move rule and numerical issues
- moving attempts to move samples ～ current target
  - _can improve essu_ by allowing samples to jump arbitrarily
    - jumps can be global or local, single or multiple times
    - weights are unchanged, same as reweight w/ unchanged target
    - ess improves by deduplication of samples (w/ same weights)
    - ess improvement depends on weights of deduplicated samples
      - if weights are uniform, then each move can be +1 ess
      - zero-weight samples can not improve ess at all
  - can fail to move samples at all, meaning failure to improve ess
    - implies low acceptance rate for metropolis-hastings jump proposals
  - can move samples but fail to move sufficiently to ～ current target
    - markov chain can fail to mix into its stationary distribution (current target)
    - can happen even if all samples are moved many times
  - may be subject to slow-moving or even non-moving samples
    - these are problematic theoretically, but some slack is desirable
  - moving rule `essu<J/2 || accepts<J` can ensure reasonable movement while allowing some slow-moving or even non-moving samples
- two hypothetical scenarios:
  - initial target is final posterior (`_weight` ~likelihood)
    - initial weights (~likelihood) will be extremely skewed (= low ess)
    - resampling will restore uniform weights but duplicate heavily (= still low ess)
    - burden is on move steps to increase ess & move samples to target
      - additional reweighting keeps uniform weights since target is final
      - additional resampling w/ uniform weights can only hurt ess
        - can be avoided easily based on ess (e.g. < 1/2 essu)
      - many move steps may be needed to ensure mixing w/ large ess
        - move steps may be interleaved with reweighting steps
  - initial target is ~prior (`_weight` ~uniform)
    - evolves from ~prior to ~posterior (`_weight` ~likelihood)
    - target evolution can help maintain samples ～ target w/ good (~1/2) ess
      - may require fewer or faster/simpler (e.g. symmetric) moves overall
    - process should still be robust to advancing target without full mixing
      - markov chain mixing diagnostics should be used at least in outer loop