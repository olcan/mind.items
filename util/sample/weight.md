#util/sample/weight function defines weighted models `Q(X) âˆ P(X) Ã— W(X)`
- unnormalized weights specified on finite sample of points
  - points = executions of sampling context `sample(function)`
  - points `(xâ‚™)` drawn from prior `P` instead of posterior `Q`
    - weights `w(x) âˆ q(x)/p(x)` necessary for _unbiased integrals_
    - weights _increase variance_ `Ã— NÃ—E[WÂ²] âˆˆ [1,N]` to eliminate bias
      - equivalently _reduce (effective) sample size_ `Ã— 1/(NÃ—E[WÂ²]) âˆˆ [1/N,1]`
        - can be derived as `NÃ—Var(weighted_mean)/Var(mean)` for i.i.d. r.v.
        - considered approximation of `NÃ—Var(target)/MSE(sample)`
        - see [Rethinking the Effective Sample Size](https://arxiv.org/abs/1809.04129) for technical details
    - weights can be converted random counts by resampling
      - considered "unweighted" despite duplication
      - samples can be _moved_ to reduce duplication
  - goal: _unweighted unduplicated_ sample from posterior `Q`
- defines _posterior_ in general sense of a _weighted prior_
  - interpretable as conditional `P(X|cond)` in two cases:
    - _likelihood weights_ `W(X) âˆ P(cond|X) = E[ðŸ™(cond|X)]`
    - _indicator weights_ `W(X) âˆ ðŸ™(cond|X)`

 sets or updates (_reweights_) sample weights to help guide samples towards a #posterior_sample. Some interpretations:
- [Discrete measure](https://en.wikipedia.org/wiki/Discrete_measure) Î¼=âˆ‘w(Î¸â‚™)Î´(Î¸â‚™,Î¸) ~ wÂ·P where Î¸â‚™~P is sample distribution.
  - Measure wÂ·P can be implicit, e.g. as stationary dist. of [markov chains](#random/methods/move).
- [Importance weights](https://en.wikipedia.org/wiki/Importance_sampling) for [efficient](https://en.wikipedia.org/wiki/Efficiency_(statistics)) and [bias/variance-controlled](https://en.wikipedia.org/wiki/Biasâ€“variance_tradeoff) estimators of the form Î·=(1/N)âˆ‘f(Î¸â‚™)w(Î¸â‚™) â†’ E[f;wÂ·P]=âˆ«f(Î¸)w(Î¸)p(Î¸)dÎ¸. Some [Posterior](#posterior_inference) [MVUEs](https://en.wikipedia.org/wiki/Minimum-variance_unbiased_estimator):
  - MVUE for E[f;Q] is [achieved by](https://en.wikipedia.org/wiki/Importance_sampling#Application_to_simulation) p(Î¸)âˆq(Î¸)|f(Î¸)| and w(Î¸)=q(Î¸)/p(Î¸)âˆ1/|f(Î¸)|.
  - MVUE for Q(A)=E[ðŸ™[Î¸âˆˆA];Q] is achieved by p(Î¸)âˆq(Î¸|A) and w(Î¸)=1.
  - MVUE for Q(Î¸) is achieved by p(Î¸)=q(Î¸) and w(Î¸)=1.
    - If p(Î¸)=Ï€(Î¸) _fixed_, then w(Î¸)=q(Î¸)/Ï€(Î¸), but Var[Î·] âˆ q(Î¸)Â²/Ï€(Î¸)Â².
    - Can lead to degenerate weights, e.g. in likelihood-weighted prior.
    - [Resampling](#random/methods/sample) resets w=1 but turns variance into bias.
    - [Markov chains](#random/methods/move) can be defined for Pâ†’Q.
- Finite-sample approximation of likelihood
  - Observation indicator (0/1) weights lead to [rejection sampling](https://en.wikipedia.org/wiki/Rejection_sampling)
  - Integrating indicators over infinite sample leads to likelihood
- Non-likelihood weights as data/model augmentation

#random/methods/update method updates samples towards #posterior_sample using a sequence of #random/methods/weight, #random/methods/sample, and #random/methods/move operations. These operations can be interpreted as steps in a [Sequential Monte Carlo](https://en.m.wikipedia.org/wiki/Particle_filter) (SMC) algorithm applied to approximate posterior inference in the sense of [Approximate Bayesian Computation](https://en.wikipedia.org/wiki/Approximate_Bayesian_computation) (ABC), a.k.a. Likelihood-free Inference (LFI). For technical background and derivation, see [ABC Samplers](https://arxiv.org/abs/1802.09650) and in particular the discussion leading up to Algorithm 8 (ABC-SMC). See more intuition and tips, see #random/methods/update/notes.

#random/methods/update/notes
- weights `wJ` are NOT same as `_weight`
  - relative weight of NEXT target under CURRENT target
  - target is `Ï†J=prior*_weight`, sometimes called posterior
  - reweighting _assumes_ samples ï½ž current target
    - if true, guarantees unbiased integrals w.r.t. next target
  - reweighting w/ same target should leave `wJ` unchanged
- at initial sampling, target is "prior" and weights uniform
  - non-uniform initial weights are allowed if sample is NOT from prior
  - in that case `_sample_weighted` must be set to true
- reweighting can shift target from current to next
  - if target does not shift, weights are unchanged
  - once target=posterior, weights should remain unchanged
  - if target shifts too much, weights can become heavily skewed
    - this can happen over multiple steps if no resampling in between
  - skewed weights cause high integral variance and small ess
- resampling forces uniform weights by converting weights â†’ counts
  - samples âˆ weights â‡’ ï½ž _discrete_ approximation to current target
  - can be interpreted as _global_ moves, restricted by current sample
    - small-weight samples can _jump_ globally to higher-weight ones
  - uniform weights minimize integral variance but introduce/increase bias
    - bias is random, due to sampling, and _baked into_ sample
  - _hurts ("shrinks") essu_ by factor ~1/2, or ~k/(k+1) for k'th resampling
  - _can improve ess_ by matching essu _after shrinkage_
    - guarantees ess=essu (âŸ¹ esr=1) _post-shrinkage_
    - can also hurt ess, which can be _higher_ than essu
      - implies new weights _less skewed_ since last resampling
  - resampling should be avoided when essr > 1/2
    - more likely to hurt ess AND essu due to duplication
  - smart resampling _can increase ess gradually to its maximum_
    - maximum essâ†’J requires uniform weights from resampling
    - once target is stable, essr stays ~1 after reweighting
    - effective moves s.t. essâ†’J can then allow essâ†’essuâ†’J
    - resampling rule `essr < essu/J` can be sufficient
      - assuming move rule ensures `essu>J/2`, see below
      - `essr<clip(essu/J,.5,1)` robust to move rule and numerical issues
- moving attempts to move samples ï½ž current target
  - _can improve essu_ by allowing samples to jump arbitrarily
    - jumps can be global or local, single or multiple times
    - weights are unchanged, same as reweight w/ unchanged target
    - ess improves by deduplication of samples (w/ same weights)
    - ess improvement depends on weights of deduplicated samples
      - if weights are uniform, then each move can be +1 ess
      - zero-weight samples can not improve ess at all
  - can fail to move samples at all, meaning failure to improve ess
    - implies low acceptance rate for metropolis-hastings jump proposals
  - can move samples but fail to move sufficiently to ï½ž current target
    - markov chain can fail to mix into its stationary distribution (current target)
    - can happen even if all samples are moved many times
  - may be subject to slow-moving or even non-moving samples
    - these are problematic theoretically, but some slack is desirable
  - moving rule `essu<J/2 || accepts<J` can ensure reasonable movement while allowing some slow-moving or even non-moving samples
- two hypothetical scenarios:
  - initial target is final posterior (`_weight` ~likelihood)
    - initial weights (~likelihood) will be extremely skewed (= low ess)
    - resampling will restore uniform weights but duplicate heavily (= still low ess)
    - burden is on move steps to increase ess & move samples to target
      - additional reweighting keeps uniform weights since target is final
      - additional resampling w/ uniform weights can only hurt ess
        - can be avoided easily based on ess (e.g. < 1/2 essu)
      - many move steps may be needed to ensure mixing w/ large ess
        - move steps may be interleaved with reweighting steps
  - initial target is ~prior (`_weight` ~uniform)
    - evolves from ~prior to ~posterior (`_weight` ~likelihood)
    - target evolution can help maintain samples ï½ž target w/ good (~1/2) ess
      - may require fewer or faster/simpler (e.g. symmetric) moves overall
    - process should still be robust to advancing target without full mixing
      - markov chain mixing diagnostics should be used at least in outer loop